{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Stage-by-stage training with:\n",
    "- Stage 1: DnCNN (denoising) + color/brightness losses\n",
    "- Stage 2: Background Removal\n",
    "- Stage 3: Simple MPRNet-based Deblurring\n",
    "Then end-to-end fine-tuning (StarEnhancementNet).\n",
    "\n",
    "Datasets assumed in:\n",
    "  ../stage1_denoising/{train,val}/{input,target}\n",
    "  ../stage2_bg_removed/{train,val}/{input,target}\n",
    "  ../stage3_final/{train,val}/{input,target}\n",
    "\"\"\"\n",
    "\n",
    "# Stage1: DnCNN for Denoising\n",
    "class DnCNN(nn.Module):\n",
    "    def __init__(self, image_channels=3, num_features=64, num_layers=17):\n",
    "        super(DnCNN, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(image_channels, num_features, kernel_size=3, padding=1, bias=False))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Conv2d(num_features, num_features, kernel_size=3, padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(num_features))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(num_features, image_channels, kernel_size=3, padding=1, bias=False))\n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = self.dncnn(x)\n",
    "        return x - noise\n",
    "\n",
    "# Stage2: Background Removal\n",
    "class BackgroundRemoval(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "    def forward(self, x):\n",
    "        bg = self.conv1(x)\n",
    "        bg = self.relu(bg)\n",
    "        bg = self.conv2(bg)\n",
    "        return x - bg\n",
    "    \n",
    "# Stage 3: Simple MPRNet-inspired Deblurring\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class SimpleMPRNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, base_channels=64):\n",
    "        super(SimpleMPRNet, self).__init__()\n",
    "        self.conv1 = conv_block(in_channels, base_channels)\n",
    "        self.conv2 = conv_block(base_channels, base_channels*2)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = conv_block(base_channels*2, base_channels*4)\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.deconv1 = conv_block(base_channels*4, base_channels*2)\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.deconv2 = conv_block(base_channels*2, base_channels)\n",
    "        self.conv_last = nn.Conv2d(base_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(self.pool(x1))\n",
    "        x_b = self.bottleneck(self.pool(x2))\n",
    "        x_up1 = self.up1(x_b)\n",
    "        x_d1 = self.deconv1(x_up1)\n",
    "        if x_d1.shape == x2.shape:\n",
    "            x_d1 = x_d1 + x2\n",
    "        x_up2 = self.up2(x_d1)\n",
    "        x_d2 = self.deconv2(x_up2)\n",
    "        if x_d2.shape == x1.shape:\n",
    "            x_d2 = x_d2 + x1\n",
    "        out = self.conv_last(x_d2)\n",
    "        return out\n",
    "\n",
    "# Combined Pipeline\n",
    "class StarEnhancementNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stage1 = DnCNN(image_channels=3, num_features=64, num_layers=17)\n",
    "        self.stage2 = BackgroundRemoval(3)\n",
    "        self.stage3 = SimpleMPRNet(3,3,64)\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# PairedDataset for input-target pairs\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, inp_dir, tgt_dir, transform=None):\n",
    "        self.inp_dir = inp_dir\n",
    "        self.tgt_dir = tgt_dir\n",
    "        self.transform = transform\n",
    "        self.files = sorted(os.listdir(self.inp_dir))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inp_filename = self.files[idx]\n",
    "        inp_path = os.path.join(self.inp_dir, inp_filename)\n",
    "        tgt_path = os.path.join(self.tgt_dir, inp_filename)\n",
    "\n",
    "        inp_bgr = cv2.imread(inp_path)\n",
    "        if inp_bgr is None:\n",
    "            raise ValueError(f\"Could not read input image {inp_path}\")\n",
    "        tgt_bgr = cv2.imread(tgt_path)\n",
    "        if tgt_bgr is None:\n",
    "            raise ValueError(f\"Could not read target image {tgt_path}\")\n",
    "\n",
    "        inp_rgb = cv2.cvtColor(inp_bgr, cv2.COLOR_BGR2RGB)\n",
    "        tgt_rgb = cv2.cvtColor(tgt_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            inp_tensor = self.transform(Image.fromarray(inp_rgb))\n",
    "            tgt_tensor = self.transform(Image.fromarray(tgt_rgb))\n",
    "        else:\n",
    "            inp_tensor = torch.from_numpy(inp_rgb).permute(2,0,1).float()/255.\n",
    "            tgt_tensor = torch.from_numpy(tgt_rgb).permute(2,0,1).float()/255.\n",
    "        return inp_tensor, tgt_tensor\n",
    "\n",
    "# Loss: L1 + brightness + color\n",
    "def brightness_loss(output, target):\n",
    "    out_mean = torch.mean(output, dim=[1,2,3])\n",
    "    tgt_mean = torch.mean(target, dim=[1,2,3])\n",
    "    return torch.mean(torch.abs(out_mean - tgt_mean))\n",
    "\n",
    "def color_consistency_loss(output, target):\n",
    "    out_mean = output.mean(dim=[2,3])\n",
    "    tgt_mean = target.mean(dim=[2,3])\n",
    "    return torch.mean(torch.abs(out_mean - tgt_mean))\n",
    "\n",
    "l1_criterion = nn.L1Loss()\n",
    "brightness_weight = 0.05\n",
    "color_weight = 0.05\n",
    "\n",
    "def compute_loss_with_color_brightness(out, tgt):\n",
    "    l1 = l1_criterion(out, tgt)\n",
    "    b_loss = brightness_loss(out, tgt)\n",
    "    c_loss = color_consistency_loss(out, tgt)\n",
    "    return l1 + brightness_weight*b_loss + color_weight*c_loss\n",
    "\n",
    "def calculate_psnr_batch(output, target):\n",
    "    output = torch.clamp(output,0,1)\n",
    "    target = torch.clamp(target,0,1)\n",
    "    out_np = output.detach().cpu().permute(0,2,3,1).numpy()\n",
    "    tgt_np = target.detach().cpu().permute(0,2,3,1).numpy()\n",
    "    psnr_vals=[]\n",
    "    for b in range(out_np.shape[0]):\n",
    "        psnr_vals.append(psnr_single(out_np[b], tgt_np[b]))\n",
    "    return np.mean(psnr_vals)\n",
    "\n",
    "def psnr_single(img1, img2):\n",
    "    mse = np.mean((img1 - img2)**2)\n",
    "    if mse < 1e-10:\n",
    "        return 100\n",
    "    return 20*np.log10(1.0/np.sqrt(mse))\n",
    "\n",
    "def show_final_val_examples(model, loader, stage_name, epoch_num, n_examples=3):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for inp, tgt in loader:\n",
    "            inp, tgt = inp.to(device), tgt.to(device)\n",
    "            out = model(inp)\n",
    "            out = torch.clamp(out, 0, 1)\n",
    "\n",
    "            inp_np = inp[0].permute(1,2,0).cpu().numpy()\n",
    "            out_np = out[0].permute(1,2,0).cpu().numpy()\n",
    "            tgt_np = tgt[0].permute(1,2,0).cpu().numpy()\n",
    "\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "            axes[0].imshow(inp_np)\n",
    "            axes[0].set_title(\"Input\")\n",
    "            axes[0].axis(\"off\")\n",
    "\n",
    "            axes[1].imshow(out_np)\n",
    "            axes[1].set_title(\"Output\")\n",
    "            axes[1].axis(\"off\")\n",
    "\n",
    "            axes[2].imshow(tgt_np)\n",
    "            axes[2].set_title(\"GroundTruth\")\n",
    "            axes[2].axis(\"off\")\n",
    "            fig.suptitle(f\"{stage_name} - Val Example (Epoch {epoch_num}) - {count}\")\n",
    "            out_name = f\"{stage_name}_e{epoch_num}_val_{count}.png\"\n",
    "            plt.savefig(out_name)\n",
    "            plt.show()\n",
    "\n",
    "            count += 1\n",
    "            if count >= n_examples:\n",
    "                break\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_stage(\n",
    "    model, \n",
    "    stage_name,\n",
    "    train_inp_dir, train_tgt_dir,\n",
    "    val_inp_dir,   val_tgt_dir,\n",
    "    epochs=10, \n",
    "    batch_size=4,\n",
    "    lr=1e-4,\n",
    "    checkpoint=None\n",
    "):\n",
    "    print(f\"Training {stage_name} for {epochs} epochs...\")\n",
    "\n",
    "    transform = transforms.ToTensor()\n",
    "    train_ds = PairedDataset(train_inp_dir, train_tgt_dir, transform)\n",
    "    val_ds   = PairedDataset(val_inp_dir,   val_tgt_dir,   transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = model.to(device)\n",
    "    if checkpoint:\n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "        print(f\"Loaded checkpoint {checkpoint}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_arr = []\n",
    "    val_loss_arr = []\n",
    "    val_psnr_arr = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        # train\n",
    "        for inp, tgt in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{epochs}\", leave=False):\n",
    "            inp, tgt = inp.to(device), tgt.to(device)\n",
    "            out = model(inp)\n",
    "            loss = compute_loss_with_color_brightness(out, tgt)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_tr_loss = running_loss / len(train_loader)\n",
    "        train_loss_arr.append(avg_tr_loss)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_val_psnr = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inp, tgt in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\", leave=False):\n",
    "                inp, tgt = inp.to(device), tgt.to(device)\n",
    "                out = model(inp)\n",
    "                vloss = compute_loss_with_color_brightness(out, tgt)\n",
    "                running_val_loss += vloss.item()\n",
    "\n",
    "                batch_psnr = calculate_psnr_batch(out, tgt)\n",
    "                running_val_psnr += batch_psnr\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        avg_val_psnr = running_val_psnr / len(val_loader)\n",
    "        val_loss_arr.append(avg_val_loss)\n",
    "        val_psnr_arr.append(avg_val_psnr)\n",
    "\n",
    "        if ((epoch+1) % 50 == 0) or (epoch+1 == epochs):\n",
    "            print(f\"[{stage_name}] Epoch {epoch+1}/{epochs} => \"\n",
    "                  f\"TrainLoss={avg_tr_loss:.4f}, ValLoss={avg_val_loss:.4f}, ValPSNR={avg_val_psnr:.2f} dB\")\n",
    "            # save checkpoint\n",
    "            torch.save(model.state_dict(), f\"{stage_name.lower()}_e{epoch+1}.pth\")\n",
    "            print(f\"Saved checkpoint => {stage_name.lower()}_e{epoch+1}.pth\")\n",
    "            show_final_val_examples(model, val_loader, stage_name, epoch+1)\n",
    "\n",
    "    final_model_path = f\"{stage_name.lower()}_e{epochs}.pth\"\n",
    "    if checkpoint is not None:\n",
    "        final_model_path = f\"{stage_name.lower()}_finetuned_e{epochs}.pth\"\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Saved final model => {final_model_path}\")\n",
    "\n",
    "    # Loss & PSNR curves\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    # loss\n",
    "    axes[0].plot(range(1, epochs+1), train_loss_arr, label=\"TrainLoss\")\n",
    "    axes[0].plot(range(1, epochs+1), val_loss_arr,   label=\"ValLoss\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(f\"{stage_name} Loss\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # psnr\n",
    "    axes[1].plot(range(1, epochs+1), val_psnr_arr, label=\"ValPSNR\", color='orange')\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"PSNR (dB)\")\n",
    "    axes[1].set_title(f\"{stage_name} Validation PSNR\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    fig.suptitle(f\"{stage_name} - Loss & PSNR\")\n",
    "    plt.tight_layout()\n",
    "    save_fig_path = f\"{stage_name.lower()}_loss_psnr_e{epochs}.png\"\n",
    "    plt.savefig(save_fig_path)\n",
    "    plt.show()\n",
    "\n",
    "    show_final_val_examples(model, val_loader, stage_name, epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Fine-tune\n",
    "def fine_tune_entire_pipeline(net, epochs=5, lr=1e-5):\n",
    "    print(f\"Fine-tuning entire pipeline for {epochs} epochs...\")\n",
    "    final_inp_dir = \"../stage3_final-mid/train/input\"\n",
    "    final_tgt_dir = \"../stage3_final-mid/train/target\"\n",
    "\n",
    "    transform = transforms.ToTensor()\n",
    "    train_ds = PairedDataset(final_inp_dir, final_tgt_dir, transform)\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_arr = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running = 0.0\n",
    "        for inp, tgt in tqdm(train_loader, desc=f\"FineTune E{epoch+1}/{epochs}\", leave=False):\n",
    "            inp, tgt = inp.to(device), tgt.to(device)\n",
    "            out = net(inp)\n",
    "            loss = compute_loss_with_color_brightness(out, tgt)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item()\n",
    "        avg_tr_loss = running / len(train_loader)\n",
    "        train_loss_arr.append(avg_tr_loss)\n",
    "\n",
    "        # Print only every 50 epochs or final\n",
    "        if ((epoch+1) % 50 == 0) or (epoch+1 == epochs):\n",
    "            avg_psnr = calculate_psnr_batch(out, tgt)\n",
    "            print(f\"[FineTune] Epoch {epoch+1}/{epochs} => TrainLoss={avg_tr_loss:.4f}, PSNR={avg_psnr:.2f} dB\")\n",
    "            torch.save(net.state_dict(), f\"final_pipeline_finetuned_e{epoch+1}.pth\")\n",
    "            print(f\"Saved checkpoint => final_pipeline_finetuned_e{epoch+1}.pth\")\n",
    "            show_final_val_examples(net, train_loader, \"FinalPipeline\", epoch+1)\n",
    "\n",
    "    final_model_path = f\"final_pipeline_large_finetuned_e{epochs}.pth\"\n",
    "    torch.save(net.state_dict(), final_model_path)\n",
    "    print(f\"Saved final pipeline => {final_model_path}\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, epochs+1), train_loss_arr, label=\"TrainLoss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Fine-tune End-to-End Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"finetune_loss_e{epochs}.png\")\n",
    "    plt.show()\n",
    "\n",
    "    show_final_val_examples(net, train_loader, \"FinalPipeline\", epochs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    epoch_num = 800\n",
    "    # Stage1: DnCNN for Denoising\n",
    "    from_stage1 = DnCNN(image_channels=3, num_features=64, num_layers=17)\n",
    "    from_stage1 = train_stage(\n",
    "        from_stage1, \n",
    "        stage_name=\"Stage1_DnCNN\",\n",
    "        train_inp_dir=\"../stage1_denoising-mid/train/input\",\n",
    "        train_tgt_dir=\"../stage1_denoising-mid/train/target\",\n",
    "        val_inp_dir=\"../stage1_denoising-mid/val/input\",\n",
    "        val_tgt_dir=\"../stage1_denoising-mid/val/target\",\n",
    "        epochs=epoch_num,\n",
    "        batch_size=8,\n",
    "        lr=1e-4\n",
    "    )\n",
    "\n",
    "    # Stage2: BG Removal\n",
    "    from_stage2 = BackgroundRemoval(channels=3)\n",
    "    from_stage2 = train_stage(\n",
    "        from_stage2,\n",
    "        stage_name=\"Stage2_BGRemoval\",\n",
    "        train_inp_dir=\"../stage2_bg_removed-mid/train/input\",\n",
    "        train_tgt_dir=\"../stage2_bg_removed-mid/train/target\",\n",
    "        val_inp_dir=\"../stage2_bg_removed-mid/val/input\",\n",
    "        val_tgt_dir=\"../stage2_bg_removed-mid/val/target\",\n",
    "        epochs=epoch_num,\n",
    "        batch_size=8,\n",
    "        lr=1e-4\n",
    "    )\n",
    "\n",
    "    # Stage3: Deblurring\n",
    "    from_stage3 = SimpleMPRNet(in_channels=3, out_channels=3, base_channels=64)\n",
    "    from_stage3 = train_stage(\n",
    "        from_stage3,\n",
    "        stage_name=\"Stage3_Deblurring\",\n",
    "        train_inp_dir=\"../stage3_final-mid/train/input\",\n",
    "        train_tgt_dir=\"../stage3_final-mid/train/target\",\n",
    "        val_inp_dir=\"../stage3_final-mid/val/input\",\n",
    "        val_tgt_dir=\"../stage3_final-mid/val/target\",\n",
    "        epochs=1000,\n",
    "        batch_size=8,\n",
    "        lr=1e-5,\n",
    "        # checkpoint=\"stage3_deblurring_e1000.pth\"\n",
    "    )\n",
    "\n",
    "    # Fine-tune entire pipeline\n",
    "    net = StarEnhancementNet()\n",
    "    net.stage1.load_state_dict(torch.load(f\"stage1_dncnn_e{500}.pth\"))\n",
    "    net.stage2.load_state_dict(torch.load(f\"stage2_bgremoval_e{800}.pth\"))\n",
    "    net.stage3.load_state_dict(torch.load(f\"./good-saved/stage3_deblurring_finetuned_e{500}.pth\"))\n",
    "    net = net.to(device)\n",
    "    fine_tune_entire_pipeline(net, epochs=100, lr=1e-4)\n",
    "    print(\"All training done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mprnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
